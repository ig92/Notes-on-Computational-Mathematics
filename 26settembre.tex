%chktex-file 36
%chktex-file 23
%chktex-file 10
%chktex-file 17
%chktex-file 9
%chktex-file 11

\documentclass[computationalMathematics.tex]{subfiles}

\begin{document}

%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%
\section{26th of September 2018 --- F. Poloni}
%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%

In the previous lecture we introduced some sufficent conditions for matrix orthogonality.

\begin{theorem}
  Let $U \in M(n, \R)$ be an orthogonal matrix and let $x \in \R^n$. Then $\norm{Ux} = \norm{x}$.
\end{theorem}

\begin{proof}
  $\norm{Ux} = \tr{(Ux)} \cdot (Ux) \numeq{(1)} \tr{x} \tr{U} U x = \tr{x} I_n x = \tr{x} x = \norm{x}$

  where $\numeq{(1)}$ follows from the definition of transpose of a product.
\end{proof}

Geometrically, an orthogonal matrix represents a symmetry or a rotations and these operations do not alter the sixe of vectors.

\begin{definition}[Orthonormality]
  Let $x, y \in R^n$ we say that $x$ and $y$ are \textbf{orthonormal} if $\ps{x}{y} = 0$ and $\norm{x} = \norm{y} = 1$.
\end{definition}

\begin{proposition}
  Let us take $U \in M(n, \R)$ such that $U$ is orthogonal. Then its columns $U^1, U^2, \ldots, U^n$ are \textbf{orthonormal} and the same holds for its rows.
  \[
  \tr{U^i} U^j = \begin{cases}
    1 \text{ if } i=j\\
    0 \text{ otherwise}
  \end{cases}
  \]
  and
  \[
    U_i \tr{U_j} = \begin{cases}
    1 \text{ if } i=j\\
    0 \text{ otherwise}
  \end{cases}
  \]
\end{proposition}

\begin{proposition}
  Let $U, V \in M(n, \R)$, such that $U$ and $V$ are orthogonal, then $U \cdot V$ is orthogonal.
\end{proposition}
\begin{proof}
  $\tr{(UV)} \cdot (UV) = \tr{V} \tr{U} U V = \tr{V} I_n V = \tr{V} V = I_n$
\end{proof}

\begin{definition}[Eigenvectors and eigenvalues]
  Let $A \in M(n, \R)$ and let $x \in \R^n$ and $\lambda \in \R$.

  If $Ax = \lambda x$ we say that $x$ is an \textbf{eigenvector} of \textbf{eigenvalue} $\lambda$.
\end{definition}

\begin{proposition}
  Let $A \in T(n, \R)$ (real triangular matrix). The eigenvalues of $A$ are the scalars on the diagonal.
\end{proposition}

Under some conditions we may obtaine a \textbf{diagonal form} of $A$, namely:
\[
  A = V \Lambda \inv{V} = \begin{pmatrix}
    \\
    V^1 & V^2 & \cdots & V^n\\
    \\
  \end{pmatrix}
  \cdot 
  \begin{pmatrix}
    \lambda_1\\
    &\lambda_2\\
    && \ddots\\
    &&&&&\lambda_n\\
  \end{pmatrix}
  \cdot 
  \begin{pmatrix}
    w_1\\
    w_2\\
    \vdots\\
    w_n\\
  \end{pmatrix}
\]

where $\forall i=1, \ldots, n~v_i$ are eigenvectors of $A$ of eigenvalue $\lambda_i$.

This is due to the fact that $\forall B \in R^n$ s.t. $Vx=b$, $x = \begin{pmatrix}
  0\\
  \vdots\\
  0\\
  1\\
  0\\
  \vdots\\
  0
\end{pmatrix}$, where the $1$ is found at position $i$ and so $AV^i=V \Lambda \inv{V} V^i = V \Lambda e_i = \lambda_i V^i$.

Another way to see the diagonalized form of $A$ is the following:

\begin{equation}
  \begin{split}
    A &= V \Lambda \inv{V} = \sum\limits_{i=1}^{n} V^i\lambda_i w_i\\
    &= \tallthin{$V^1$} \cdot \real{$\lambda_1$} \cdot \shortlarge{$w_1$} + \tallthin{$V^2$} \cdot \real{$\lambda_2$} \cdot \shortlarge{$w_2$} + \cdots + \tallthin{$V^n$} \cdot \real{$\lambda_n$} \cdot \shortlarge{$w_n$}
  \end{split}
\end{equation}

\syntax{Notice that in Matlab the eigenvalues and eigenvectors of a matrix are computed using the command \texttt{[V, Lambda] = eig(U)} and this operation has a computational complexity of $O(n^3)$.}

Notice that not all matrices $A \in M(n, \R)$ allow a diagonal decomposition.
It may happen that such a matrix is diagonalizable in $\mathds{C}$ and its eigenvalues are complex.

\begin{proposition}
  If $A \in M(n, \R)$ is diagonalizable (aka may be written as $A = V \Lambda \inv{V}$) $A^k x = \sum\limits_{i=1}^{n} \lambda_i^k \alpha_i V^i$, for some $\alpha_i \in \R$.
\end{proposition}

\begin{proof}
  \begin{description}
    \item[{\sc Algebraic view point:}]
      Let us write $x$ in the base of $R^n$ made of the linearly independent columns of $V$:
  \[
x = V^1 \alpha_1 + V^2 \alpha_2 + \cdots + V^n \alpha_n
  \]
  for some $\alpha_i \in \R$.

  \begin{equation}
    \begin{split}
      Ax &= A \cdot ( V^1 \alpha_1 + V^2 \alpha_2 + \cdots + V^n \alpha_n)\\
      &= A V_1 \alpha_1 + A V_2 \alpha_2 + \cdots + A V_n \alpha_n\\
      & = \lambda_1 V^1 \alpha_1 + \lambda_2 V^2 \alpha_2 + \cdots + \lambda_n V^n \alpha_n\\
      &= V^1 (\lambda_1 \alpha_1) + V^2 (\lambda_2 \alpha_2) + \cdots + V^n (\lambda_n \alpha_n)\\
    \end{split}
  \end{equation}

  Then
 \begin{equation}
    \begin{split}
      A^2 x &= A \cdot \Big (V^1 (\lambda_1 \alpha_1) + V^2 (\lambda_2 \alpha_2) + \cdots + V^n (\lambda_n \alpha_n) \Big )\\
      &= A V^1 \lambda_1 \alpha_1 + A V^2 \lambda_2 \alpha_2 + \cdots + A V^n \lambda_n \alpha_n\\
      &= {\lambda_1}^2 V^1 \alpha_1 + {\lambda_2}^2 V^2 \alpha_2 + \cdots + A {\lambda_n}^2 V^n \alpha_n\\
    \end{split}
  \end{equation}
Inductively, we have proved the theorem.
  \item[{\sc Linear algebra view point:}]
    \begin{equation}
      \begin{split}
        A^k x &= A \cdot A \cdot \ldots \cdot A \cdot x\\
        &=V \Lambda \cancel{\inv{V}} \cancel{V} \Lambda \cancel{\inv{V}} \ldots \cancel{V} \Lambda \inv{V} x\\
        &= V \Lambda^k \inv{V} x\\
        &=V \begin{pmatrix}
          {\lambda_1}^k\\
          & {\lambda_2}^k\\
          && \ddots\\
          &&&& {\lambda_n}^k\\
        \end{pmatrix}
        \inv{V} x\\
        &=V \begin{pmatrix}
          {\lambda_1}^k\\
          & {\lambda_2}^k\\
          && \ddots\\
          &&&& {\lambda_n}^k\\
        \end{pmatrix}
        \cdot \begin{pmatrix}
          {\alpha_1}\\
          {\alpha_2}\\
          \ddots\\
          {\alpha_n}\\
        \end{pmatrix}
      \end{split}
    \end{equation}

  \end{description}
\end{proof}

Thanks to this proposition we can prove the following

\begin{theorem}
  Let $A \in M(n, \R)$. If $\forall \lambda_i$ eigenvalues of $A$ $\abs{\lambda_i} < 1$ then $\lim\limits_{k \to \infty} A^k x = 0$.
\end{theorem}

\begin{theorem}
  Let $A \in M(n, \R)$. If $\forall \lambda_i$ eigenvalues of $A$ $\abs{\lambda_i} < \abs{\lambda_1}$ then $A^k x \approx V^1 {\lambda_1}^k \alpha_1$.
\end{theorem}

\begin{proposition}
  Let $A \in M(n, \R)$ be a diagonalizable matrix and let 
  \[
    A=V\Lambda \inv{V} = \begin{pmatrix}
    \\
    V^1 & V^2 & \cdots & V^n\\
    \\
  \end{pmatrix}
  \cdot 
  \begin{pmatrix}
    \lambda_1\\
    &\lambda_2\\
    && \ddots\\
    &&&&&\lambda_n\\
  \end{pmatrix}
  \cdot 
  \begin{pmatrix}
    w_1\\
    w_2\\
    \vdots\\
    w_n\\
  \end{pmatrix}
\]

  Let us now consider a reordering of $V$'s columns and apply the same permutations to the ``diagonal vector'' of $\Lambda$ such that
  
\[
    \hat{V} = \begin{pmatrix}
    \\
    V^2 & V^1 & V^3 \cdots & V^n\\
    \\
  \end{pmatrix}
  \text{ and }
  \hat{\Lambda}= \begin{pmatrix}
    \lambda_2\\
    &\lambda_1\\
    &&\lambda_3\\
    &&& \ddots\\
    &&&&&&\lambda_n\\
  \end{pmatrix}
\]

  A can be diagonalized through such $\hat{V}$ and $\hat{\Lambda}$: $A = V \Lambda \inv{V} = \hat{V} \hat{\Lambda} \inv{\hat{V}}$.
\end{proposition}

Moreover, in the case of repeated eigenvalues

\begin{proposition}
  Let $A \in M(n, \R)$ a diagonalizable matrix such that $A = V \Lambda \inv{V}$, where $\lambda_1 = \lambda_2$ (without loss of generality). Then $V$ can be replaced by $\widetilde{V} = \begin{pmatrix}
    \\
    \\
    V^1 + V^2 & V^1 - V^2 & V^3 & \cdots & V^n\\
    \\
    \\
  \end{pmatrix}$.
\end{proposition}

\begin{theorem}[Spectral theorem]
  Let $A \in S(n, \R)$ ($A$ is a real symmetric matrix). Then $A$ is diagonalizable, its eigenvalues are real numbers and $V$ is an orthogonal matrix.
\end{theorem}

\begin{proposition}
  Let $A \in S(n, \R)$ and let $x \in \R^n$. $\lambda_{\text{min}} \sqrnorm{x} \le \tr{x}Ax \le \lambda_{\text{max}} \sqrnorm{x}$, where $\lambda_{\text{max}}$ and $\lambda_{\text{min}}$ are respectively the eigenvalue of maximum value and the eigenvalue fo minimum value.
\end{proposition}

\begin{proof}
  \begin{description}
    \item[{\sc Diagonal $A$:}]
      $\tr{x}Ax = \tr{x} \cdot \begin{pmatrix}
    \lambda_2\\
    &\lambda_1\\
    &&\lambda_3\\
    &&& \ddots\\
    &&&&&&\lambda_n\\
    \end{pmatrix}
      \cdot x = \lambda_1 {x_1}^2 + \lambda_2 {x_2}^2 + \cdots + \lambda_n {x_n}^2$
  It is obvious that this sum is bounded by:
  
  \[
    \lambda_{\text{min}} \cdot ({x_1}^2 +{x_2}^2 + \cdots + {x_n}^2) \le  \lambda_1 {x_1}^2 + \lambda_2 {x_2}^2 + \cdots + \lambda_n {x_n}^2 \le \lambda_{\text{max}} \cdot ({x_1}^2 +{x_2}^2 + \cdots + {x_n}^2)
  \]

      The following holds: $ \lambda_{\text{min}} \cdot ({x_1}^2 +{x_2}^2 + \cdots + {x_n}^2) =  \lambda_{\text{min}} \cdot \tr{x} x =  \lambda_{\text{min}} \cdot \sqrnorm{x}$ and, on the other hand, $ \lambda_{\text{max}} \cdot ({x_1}^2 +{x_2}^2 + \cdots + {x_n}^2) =  \lambda_{\text{max}} \cdot \tr{x} x =  \lambda_{\text{max}} \cdot \sqrnorm{x}$ and this proves the fact in the special case of diagonal matrix $A$.
    \item[{\sc General case:}]
      Let us represent $A$ through its eigendecomposition: $A = U \Lambda \inv{U} = U \Lambda \tr{U}$, where $U$ is an orthogonal matrix.

      $\tr{x}Ax = \tr{x} U \Lambda \tr{U} x \numeq{(1)} \tr{y} \Lambda y$

      where $\numeq{(1)}$ is due to the change of variable $y = \tr{U} x$ (that implies $\tr{y} = \tr{x} U$).

      By the same argument used in the diagonal case,

      $\lambda_{\text{min}} \cdot \sqrnorm{y} \le \tr{y} \Lambda y \le \lambda_{\text{max}} \cdot \sqrnorm{y}$.

      Now the point is that if we can replace $\sqrnorm{y}$ with $\sqrnorm{x}$ we have proved the theorem.
      In fact this is true, due to the orthogonality of matrix $U$.
  \end{description}
\end{proof}

\begin{corollary}
  Let $A \in S(n, \R)$ and let $x \in \R^n$. If $x \neq 0$, $\lambda_{\text{min}} \le \frac{\tr{x}Ax}{\sqrnorm{x}} \le \lambda_{\text{max}}$, where $\lambda_{\text{max}}$ and $\lambda_{\text{min}}$ are respectively the eigenvalue of maximum value and the eigenvalue fo minimum value.
\end{corollary}

\begin{definition}[Positive (semi)definite]
  Let $A \in S(n, \R)$. We say that $A$ is \textbf{positive semidefinite} if $\forall x \in R^n$ $\tr{x} A x \ge 0$.  
 
  On the other hand,$A$ is termed \textbf{positive definite} if $\tr{x} A x > 0 \forall x \in R^n \setminus \{0\}$.
\end{definition}

\begin{proposition}
  Let $A \in S(n, \R)$.
  $\forall \lambda$ eigenvalue of $A$ $\lambda \ge 0$ iff $A$ is \textbf{positive semidefinite}.

  On the other hand, all eigenvalues are \textbf{strictly} positive iff $A$ is positive definite.
\end{proposition}

\begin{proof}~\\
  \indent ($\Rightarrow$) $\tr{x} A x \ge \lambda_{\text{min}}$.

  ($\Leftarrow$)Let $v_i$ be an eigenvector of $A$.

  \indent $0 \le \tr{v_i} A v_i = \tr{v_i} \cdot ( \lambda_i v_i) = \lambda_i  \tr{v_i} v_i = \lambda_i \sqrnorm{v_i} \Rightarrow \lambda_i \ge 0$.
\end{proof}

\begin{proposition}
  Let $B \in M(m, n, \R)$, $\tr{B} B \in S(n, \R)$ is positive semidefinite.
\end{proposition}

\begin{proof}
\begin{description}
  \item[{\sc Symmetry:}] $\tr{(\tr{B}B)} = \tr{B} \cdot \tr{(\tr{B})} = \tr{B} B$.
  \item[{\sc Positive definite:}] $\tr{x} \tr{B} B x = \tr{(B x)} (Bx) = \sqrnorm{Bx} \ge 0$
\end{description}
\end{proof}

\begin{corollary}
The same holds for $B\tr{B}$, since we can define $C = \tr{B}$.
\end{corollary}

\begin{proposition}
  Let $A \in S(n, \R)$. $A \succeq 0$ and $A$ invertible iff $A$ is \textbf{strictly positive definite}.
\end{proposition}

\syntax{
  In ordert to check if a matrix $A$ is positive definite in Matlab we can look at its eigenvalues (cfr. \texttt{eig(A)}).
}
\end{document}
